{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a903afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytask='all'        #'diagnostic', 'subdiagnostic', 'superdiagnostic', 'form', 'rhythm', 'all'\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='1'\n",
    "device='cuda:0'\n",
    "\n",
    "database_csv='./ptbxl/ptbxl_database.csv'\n",
    "raw100_npy='./ptbxl/raw100.npy'\n",
    "ptbxl_folder='./ptbxl/'\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "X = pd.read_csv(database_csv, index_col='ecg_id')\n",
    "\n",
    "ZZ = np.zeros((21837,4))\n",
    "ZZ [:,0]=np.array(X['age'])\n",
    "ZZ [:,1]=np.array(X['sex'])\n",
    "ZZ [:,2]=np.array(X['height'])\n",
    "ZZ [:,3]=np.array(X['weight'])\n",
    "ZZ [np.isnan(ZZ)]=300  \n",
    "ZZ [ZZ==300]=-100\n",
    "print(type(ZZ))\n",
    "print(ZZ)\n",
    "print(ZZ.shape)\n",
    "ZZ [:,0]=ZZ [:,0]/100 \n",
    "ZZ [:,2]=ZZ [:,2]/200\n",
    "ZZ [:,3]=ZZ [:,3]/100\n",
    "\n",
    "print(ZZ)\n",
    "\n",
    "\n",
    "Y = pd.read_csv(database_csv, index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "X = np.load(raw100_npy, allow_pickle=True)\n",
    "\n",
    "def compute_label_aggregations(df, folder=ptbxl_folder, ctype='superdiagnostic'):\n",
    "\n",
    "    df['scp_codes_len'] = df.scp_codes.apply(lambda x: len(x))\n",
    "\n",
    "    aggregation_df = pd.read_csv(folder+'scp_statements.csv', index_col=0)\n",
    "\n",
    "    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:\n",
    "\n",
    "        def aggregate_all_diagnostic(y_dic):\n",
    "            tmp = []\n",
    "            for key in y_dic.keys():\n",
    "                if key in diag_agg_df.index:\n",
    "                    tmp.append(key)\n",
    "            return list(set(tmp))\n",
    "\n",
    "        def aggregate_subdiagnostic(y_dic):\n",
    "            tmp = []\n",
    "            for key in y_dic.keys():\n",
    "                if key in diag_agg_df.index:\n",
    "                    c = diag_agg_df.loc[key].diagnostic_subclass\n",
    "                    if str(c) != 'nan':\n",
    "                        tmp.append(c)\n",
    "            return list(set(tmp))\n",
    "\n",
    "        def aggregate_diagnostic(y_dic):\n",
    "            tmp = []\n",
    "            for key in y_dic.keys():\n",
    "                if key in diag_agg_df.index:\n",
    "                    c = diag_agg_df.loc[key].diagnostic_class\n",
    "                    if str(c) != 'nan':\n",
    "                        tmp.append(c)\n",
    "            return list(set(tmp))\n",
    "\n",
    "        diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]\n",
    "        if ctype == 'diagnostic':\n",
    "            df['diagnostic'] = df.scp_codes.apply(aggregate_all_diagnostic)\n",
    "            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))\n",
    "        elif ctype == 'subdiagnostic':\n",
    "            df['subdiagnostic'] = df.scp_codes.apply(aggregate_subdiagnostic)\n",
    "            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))\n",
    "        elif ctype == 'superdiagnostic':\n",
    "            df['superdiagnostic'] = df.scp_codes.apply(aggregate_diagnostic)\n",
    "            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))\n",
    "    elif ctype == 'form':\n",
    "        form_agg_df = aggregation_df[aggregation_df.form == 1.0]\n",
    "\n",
    "        def aggregate_form(y_dic):\n",
    "            tmp = []\n",
    "            for key in y_dic.keys():\n",
    "                if key in form_agg_df.index:\n",
    "                    c = key\n",
    "                    if str(c) != 'nan':\n",
    "                        tmp.append(c)\n",
    "            return list(set(tmp))\n",
    "\n",
    "        df['form'] = df.scp_codes.apply(aggregate_form)\n",
    "        df['form_len'] = df.form.apply(lambda x: len(x))\n",
    "    elif ctype == 'rhythm':\n",
    "        rhythm_agg_df = aggregation_df[aggregation_df.rhythm == 1.0]\n",
    "\n",
    "        def aggregate_rhythm(y_dic):\n",
    "            tmp = []\n",
    "            for key in y_dic.keys():\n",
    "                if key in rhythm_agg_df.index:\n",
    "                    c = key\n",
    "                    if str(c) != 'nan':\n",
    "                        tmp.append(c)\n",
    "            return list(set(tmp))\n",
    "\n",
    "        df['rhythm'] = df.scp_codes.apply(aggregate_rhythm)\n",
    "        df['rhythm_len'] = df.rhythm.apply(lambda x: len(x))\n",
    "    elif ctype == 'all':\n",
    "        df['all_scp'] = df.scp_codes.apply(lambda x: list(set(x.keys())))\n",
    "\n",
    "    return df\n",
    "\n",
    "def select_data_with_meta(XX,YY,ZZ, ctype, min_samples, outputfolder):\n",
    "    # convert multilabel to multi-hot\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    if ctype == 'diagnostic':\n",
    "        X = XX[YY.diagnostic_len > 0]\n",
    "        Y = YY[YY.diagnostic_len > 0]\n",
    "        Z = ZZ[YY.diagnostic_len > 0]\n",
    "        mlb.fit(Y.diagnostic.values)\n",
    "        y = mlb.transform(Y.diagnostic.values)\n",
    "    elif ctype == 'subdiagnostic':\n",
    "        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()\n",
    "        counts = counts[counts > min_samples]\n",
    "        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))\n",
    "        X = XX[YY.subdiagnostic_len > 0]\n",
    "        Y = YY[YY.subdiagnostic_len > 0]\n",
    "        Z = ZZ[YY.subdiagnostic_len > 0]\n",
    "        mlb.fit(Y.subdiagnostic.values)\n",
    "        y = mlb.transform(Y.subdiagnostic.values)\n",
    "    elif ctype == 'superdiagnostic':\n",
    "        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()\n",
    "        counts = counts[counts > min_samples]\n",
    "        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))\n",
    "        X = XX[YY.superdiagnostic_len > 0]\n",
    "        Y = YY[YY.superdiagnostic_len > 0]\n",
    "        Z = ZZ[YY.superdiagnostic_len > 0]\n",
    "        mlb.fit(Y.superdiagnostic.values)\n",
    "        y = mlb.transform(Y.superdiagnostic.values)\n",
    "    elif ctype == 'form':\n",
    "        # filter\n",
    "        counts = pd.Series(np.concatenate(YY.form.values)).value_counts()\n",
    "        counts = counts[counts > min_samples]\n",
    "        YY.form = YY.form.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "        YY['form_len'] = YY.form.apply(lambda x: len(x))\n",
    "        # select\n",
    "        X = XX[YY.form_len > 0]\n",
    "        Y = YY[YY.form_len > 0]\n",
    "        Z = ZZ[YY.form_len > 0]\n",
    "        mlb.fit(Y.form.values)\n",
    "        y = mlb.transform(Y.form.values)\n",
    "    elif ctype == 'rhythm':\n",
    "        # filter \n",
    "        counts = pd.Series(np.concatenate(YY.rhythm.values)).value_counts()\n",
    "        counts = counts[counts > min_samples]\n",
    "        YY.rhythm = YY.rhythm.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "        YY['rhythm_len'] = YY.rhythm.apply(lambda x: len(x))\n",
    "        # select\n",
    "        X = XX[YY.rhythm_len > 0]\n",
    "        Y = YY[YY.rhythm_len > 0]\n",
    "        Z = ZZ[YY.rhythm_len > 0]\n",
    "        mlb.fit(Y.rhythm.values)\n",
    "        y = mlb.transform(Y.rhythm.values)\n",
    "    elif ctype == 'all':\n",
    "        # filter \n",
    "        counts = pd.Series(np.concatenate(YY.all_scp.values)).value_counts()\n",
    "        counts = counts[counts > min_samples]\n",
    "        YY.all_scp = YY.all_scp.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "        YY['all_scp_len'] = YY.all_scp.apply(lambda x: len(x))\n",
    "        # select\n",
    "        X = XX[YY.all_scp_len > 0]\n",
    "        Y = YY[YY.all_scp_len > 0]\n",
    "        Z = ZZ[YY.all_scp_len > 0]\n",
    "        mlb.fit(Y.all_scp.values)\n",
    "        y = mlb.transform(Y.all_scp.values)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return X, Y, Z, y, mlb\n",
    "\n",
    "labels = compute_label_aggregations(Y, ptbxl_folder, mytask)\n",
    "\n",
    "def preprocess_signals(X_train, X_validation, X_test, outputfolder):\n",
    "    # Standardize data such that mean 0 and variance 1\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
    "\n",
    "    return apply_standardizer(X_train, ss), apply_standardizer(X_validation, ss), apply_standardizer(X_test, ss)\n",
    "\n",
    "def apply_standardizer(X, ss):\n",
    "    X_tmp = []\n",
    "    for x in X:\n",
    "        x_shape = x.shape\n",
    "        X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
    "    X_tmp = np.array(X_tmp)\n",
    "    return X_tmp\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "# Select relevant data and convert to one-hot\n",
    "data,labels,Z,Y, _ = select_data_with_meta(X, labels, ZZ, mytask, 0, '.')\n",
    "print('ptbxl labels shape is:',labels.shape)\n",
    "print('n_classes number shape is:',Y.shape)\n",
    "print(Y[0,:])\n",
    "input_shape = data[0].shape\n",
    "\n",
    "# 10th fold for testing (9th for now)\n",
    "X_test = data[labels.strat_fold == 10]\n",
    "y_test = Y[labels.strat_fold == 10]\n",
    "z_test = Z[labels.strat_fold == 10]\n",
    "# 9th fold for validation (8th for now)\n",
    "X_val = data[labels.strat_fold == 9]\n",
    "y_val = Y[labels.strat_fold == 9]\n",
    "z_val = Z[labels.strat_fold == 9]\n",
    "# rest for training\n",
    "X_train = data[labels.strat_fold <= 8]\n",
    "y_train = Y[labels.strat_fold <= 8]\n",
    "z_train = Z[labels.strat_fold <= 8]\n",
    "\n",
    "# Preprocess signal data\n",
    "X_train, X_val, X_test = preprocess_signals(X_train, X_val, X_test, '.')\n",
    "n_classes = y_train.shape[1]\n",
    "print(y_train.shape)\n",
    "print(X_train.shape) # 17441,1000,12\n",
    "print(z_train.shape) # 17441,4\n",
    "\n",
    "# 把z扩充为17441，4，12，然后和X在第二维融合在一起\n",
    "\n",
    "z_train=np.expand_dims(z_train,axis=2).repeat(12,axis=2)\n",
    "z_val=np.expand_dims(z_val,axis=2).repeat(12,axis=2)\n",
    "z_test=np.expand_dims(z_test,axis=2).repeat(12,axis=2)\n",
    "\n",
    "X_train=np.concatenate((X_train,z_train),axis=1)\n",
    "X_val=np.concatenate((X_val,z_val),axis=1)\n",
    "X_test=np.concatenate((X_test,z_test),axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "import shutil\n",
    "import datetime as dtm\n",
    "import time\n",
    "from scipy.signal import stft\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from numpy import concatenate\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import timm\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "import sklearn \n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "from scipy import signal\n",
    "from numpy import mean\n",
    "from torch.nn.modules import conv\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from math import ceil\n",
    "from torch import nn, einsum\n",
    "from enum import Enum\n",
    "import re\n",
    "import inspect\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "print('import ok')\n",
    "\n",
    "def get_np_posemb(patch_nums,emb_dim,cls_token=False):\n",
    "    out_np_pe=np.zeros((patch_nums,emb_dim))\n",
    "    for tmp_emb_index in range(emb_dim):\n",
    "        tmp_emb_k=tmp_emb_index//2\n",
    "        tmp_emb_omega=10000**(2*tmp_emb_k/emb_dim)\n",
    "        if tmp_emb_index%2==0:            \n",
    "            for tmp_patch_index in range(patch_nums):\n",
    "                out_np_pe[tmp_patch_index,tmp_emb_index]=np.sin(float(tmp_patch_index)/tmp_emb_omega)\n",
    "        elif tmp_emb_index%2==1:            \n",
    "            for tmp_patch_index in range(patch_nums):\n",
    "                out_np_pe[tmp_patch_index,tmp_emb_index]=np.cos(float(tmp_patch_index)/tmp_emb_omega)\n",
    "        else:\n",
    "            print('error pos emb!')\n",
    "    if cls_token:\n",
    "        out_np_pe=np.concatenate([np.zeros([1, emb_dim]), out_np_pe], axis=0)\n",
    "    return out_np_pe\n",
    "            \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  \n",
    "        self.fn = fn  \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Temporal_Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads  \n",
    "        project_out = not (heads == 1 and dim_head == dim)  \n",
    "        self.heads = heads  \n",
    "        self.scale = dim_head ** -0.5  \n",
    "        self.attend = nn.Softmax(dim = -1) \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        \n",
    "        x=rearrange(x, 'b (tws twn) c -> (b tws) twn c', tws = 6)\n",
    "        b, n, _, h = *x.shape, self.heads  \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)  \n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)  \n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale  \n",
    "        attn = self.attend(dots)  \n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)  \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')  \n",
    "        out=rearrange(out, '(b tws) twn c -> b (tws twn) c', tws = 6)\n",
    "        final_out=self.to_out(out)\n",
    "        return final_out  \n",
    "    \n",
    "class Leadview_Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads \n",
    "        project_out = not (heads == 1 and dim_head == dim)  \n",
    "        self.heads = heads \n",
    "        self.scale = dim_head ** -0.5  \n",
    "        self.attend = nn.Softmax(dim = -1)  \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        \n",
    "        x=rearrange(x, 'b (lgn lgs) c-> (b lgs) lgn c', lgs = 12)\n",
    "        b, n, _, h = *x.shape, self.heads  \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)  \n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)  \n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale  \n",
    "        attn = self.attend(dots)  \n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)  \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)') \n",
    "        out=rearrange(out, '(b lgs) lgn c -> b (lgn lgs) c', lgs = 12)\n",
    "        final_out=self.to_out(out)\n",
    "        return final_out      \n",
    "    \n",
    "\n",
    "class F_Attention(nn.Module):  \n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads  \n",
    "        project_out = not (heads == 1 and dim_head == dim) \n",
    "        self.heads = heads \n",
    "        self.scale = dim_head ** -0.5  \n",
    "        self.attend = nn.Softmax(dim = -1) \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False) \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        b, n, _, h = *x.shape, self.heads \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1) \n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)  \n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale \n",
    "        attn = self.attend(dots) \n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)') \n",
    "        final_out=self.to_out(out)\n",
    "        return final_out \n",
    "    \n",
    "    \n",
    "class Da_Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, mlp_dim, dropout = 0. ):\n",
    "        super().__init__()\n",
    "        heads=5\n",
    "        dim_head=dim//heads\n",
    "        self.layers = nn.ModuleList([])  \n",
    "        self.aug_shortcut=nn.Sequential(\n",
    "                nn.Linear(dim,dim),\n",
    "                nn.GELU(),\n",
    "        )\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Temporal_Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),  \n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)),  \n",
    "                PreNorm(dim, Leadview_Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), \n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)),  \n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for t_attn, t_ff, l_attn, l_ff in self.layers:\n",
    "            x = t_attn(x) + x \n",
    "            x = t_ff(x) + x        \n",
    "            x = l_attn(x) + x \n",
    "            x = l_ff(x) + x   \n",
    "        return x\n",
    "\n",
    "class F_Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, mlp_dim, dropout = 0. ):\n",
    "        super().__init__()\n",
    "        heads=5\n",
    "        dim_head=dim//heads\n",
    "        self.layers = nn.ModuleList([]) \n",
    "        self.aug_shortcut=nn.Sequential(\n",
    "                nn.Linear(dim,dim),\n",
    "                nn.GELU(),\n",
    "        )\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, F_Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), \n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) \n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x \n",
    "            x = ff(x) + x        \n",
    "        return x    \n",
    "    \n",
    "class ECGViT(nn.Module):\n",
    "    def __init__(self, *, signal_length: int=250, patch_size:int=50, num_classes:int=71, dim:int=1024, Da_depth:int=4, F_depth:int=2, mlp_dim:int=2048, channels:int = 1,leads:int=12, dropout = 0.,meta_info=0):\n",
    "        super().__init__()\n",
    "        assert signal_length % patch_size == 0, 'signal_length must be divisible by the patch size.' \n",
    "        self.leads=leads\n",
    "        self.patch_size=patch_size\n",
    "        self.meta_info=meta_info\n",
    "        num_patches = self.leads*(signal_length  // patch_size)+self.leads  \n",
    "        patch_dim = channels * patch_size *1  \n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = patch_size), \n",
    "            nn.Linear(patch_dim, dim),  \n",
    "        )\n",
    "\n",
    "        self.pos_emb_1 = nn.Parameter(torch.randn(1,num_patches,dim),requires_grad=False)\n",
    "        self.np_posemb_1=get_np_posemb(num_patches,self.pos_emb_1.shape[2],cls_token=False)\n",
    "        self.pos_emb_1.data.copy_(torch.from_numpy(self.np_posemb_1).float().unsqueeze(0))\n",
    "        \n",
    "        self.pos_emb_2 = nn.Parameter(torch.randn(1,num_patches+self.meta_info+1,dim),requires_grad=False)\n",
    "        self.np_posemb_2=get_np_posemb(num_patches+self.meta_info,self.pos_emb_2.shape[2],cls_token=True)\n",
    "        self.pos_emb_2.data.copy_(torch.from_numpy(self.np_posemb_2).float().unsqueeze(0))\n",
    "        \n",
    "        self.Da_transformer = Da_Transformer(dim, Da_depth,mlp_dim, dropout) \n",
    "        self.F_transformer = F_Transformer(dim, F_depth,mlp_dim, dropout) \n",
    "        \n",
    "        self.to_latent = nn.Identity()  \n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim), \n",
    "            nn.Linear(dim, num_classes) \n",
    "        )\n",
    "        self.maxpool_global=nn.MaxPool2d((1, 5), stride=(1, 5))\n",
    "    \n",
    "        self.meta_to_embedding=nn.Linear(self.leads,dim)\n",
    "        \n",
    "    def forward(self, insig): # bs,12,254\n",
    "        sig=insig[:,:,:-1*self.meta_info]\n",
    "        meta_sig=insig[:,:,-1*self.meta_info:].transpose(-1,-2)  # bs,4,12\n",
    "        sig1=sig.unsqueeze(1)        \n",
    "        pooled_sig=torch.zeros((sig1.shape[0],sig1.shape[1],sig1.shape[2],sig1.shape[3]+self.patch_size)).to(device)\n",
    "        pooled_sig[:,:,:,:sig1.shape[3]]=sig1\n",
    "        pooled_sig[:,:,:,sig1.shape[3]:sig1.shape[3]+self.patch_size]=self.maxpool_global(sig1)\n",
    "        x = self.to_patch_embedding(pooled_sig) \n",
    "        b, n, _ = x.shape  \n",
    "        x2 = x + self.pos_emb_1 \n",
    "        x3 = self.Da_transformer(x2)\n",
    "        meta_tokens=self.meta_to_embedding(meta_sig)\n",
    "        cls_tokens =  x3.mean(dim = 1).unsqueeze(1) \n",
    "        x4 = torch.cat((cls_tokens,meta_tokens,x3), dim=1)\n",
    "        x4_=x4+self.pos_emb_2\n",
    "        x5 = self.F_transformer(x4_) \n",
    "        x6 = x5[:, 0]    \n",
    "        x7 = self.to_latent(x6)\n",
    "        x8 = self.mlp_head(x7)\n",
    "        return x8\n",
    "\n",
    "ECGViT_model=ECGViT(num_classes = 71,dim = 160,mlp_dim = 480,Da_depth=4, F_depth=2,patch_size=50,leads=12,dropout=0.2,meta_info=4).to(device) \n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model,loss_fn, optimizer, train_loader, device, scheduler=None,warmup=False):\n",
    "    global ema\n",
    "    model.train() \n",
    "    t = time.time()\n",
    "    running_loss = None \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (in_data_, targets_) in pbar:\n",
    "        data_=in_data_[:,:1000,:].transpose(-1,-2).type(torch.FloatTensor).to(device)\n",
    "        meta_=in_data_[:,1000:,:].transpose(-1,-2).type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for tmp_seg in range(ceil((data_.shape[-1])/125-0.1)-1):\n",
    "            if (tmp_seg==0):\n",
    "                random_bias=random.randint(0,int(125/2))\n",
    "            elif (tmp_seg==(ceil((data_.shape[-1])/125-0.1)-2)):\n",
    "                random_bias=random.randint(-int(125/2),0)\n",
    "            else:\n",
    "                random_bias=random.randint(-int(125/2),int(125/2))\n",
    "\n",
    "            seg_data_=torch.cat((data_[:,:,125*tmp_seg+random_bias:125*(tmp_seg+2)+random_bias],meta_),-1)\n",
    "            preds_=model(seg_data_).type(torch.cuda.FloatTensor)\n",
    "            targets_=targets_.type(torch.FloatTensor).to(device)\n",
    "            loss=loss_fn(preds_,targets_)\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            preds_=model(seg_data_).type(torch.cuda.FloatTensor)\n",
    "            targets_=targets_.type(torch.FloatTensor).to(device)\n",
    "            loss=loss_fn(preds_,targets_)\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "        if running_loss is None:\n",
    "            running_loss = loss.item() \n",
    "        else:\n",
    "            running_loss = running_loss * .99 + loss.item() * .01           \n",
    "        description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "        pbar.set_description(description)\n",
    "        ema.update()\n",
    "\n",
    "    if warmup==True:\n",
    "        if scheduler!=None:\n",
    "            scheduler.step()   \n",
    "    return running_loss\n",
    "print('ok')\n",
    "\n",
    "\n",
    "def val_one_epoch_wEMA(epoch, model,loss_fn,  val_loader, device,scheduler):\n",
    "    global ema\n",
    "    ema.apply_shadow()\n",
    "    model.eval() \n",
    "    val_tar=0\n",
    "    mean_val_pred=0\n",
    "    max_val_pred=0\n",
    "    with torch.no_grad():\n",
    "        t = time.time()\n",
    "        running_loss = None \n",
    "        pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "        for step, (in_data_, targets_) in pbar:\n",
    "            data_=in_data_[:,:1000,:].transpose(-1,-2).type(torch.FloatTensor).to(device)\n",
    "            meta_=in_data_[:,1000:,:].transpose(-1,-2).type(torch.FloatTensor).to(device)\n",
    "            all_seg_preds_=torch.zeros((ceil((data_.shape[-1])/125-0.1)-1,targets_.shape[0],targets_.shape[1])).type(torch.FloatTensor).to(device) \n",
    "            for tmp_seg in range(ceil((data_.shape[-1])/125-0.1)-1):\n",
    "                seg_data_=torch.cat((data_[:,:,125*tmp_seg:125*(tmp_seg+2)],meta_),-1)\n",
    "                seg_preds_=model(seg_data_).type(torch.cuda.FloatTensor)\n",
    "                all_seg_preds_[tmp_seg,:,:]=seg_preds_.clone().detach().type(torch.cuda.FloatTensor) \n",
    "            agg_preds_=(all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).to(device)\n",
    "            targets_=targets_.type(torch.FloatTensor).to(device) \n",
    "            loss=loss_fn(agg_preds_,targets_)\n",
    "            if running_loss is None:\n",
    "                running_loss_list=[]\n",
    "                running_loss = loss.item()\n",
    "                running_loss_list.append(loss.item()) \n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01 \n",
    "                running_loss_list.append(loss.item()) \n",
    "            description = f'epoch {epoch} mean loss: {mean(running_loss_list):.4f}' \n",
    "            pbar.set_description(description)  \n",
    "            all_seg_preds_=torch.sigmoid(all_seg_preds_)\n",
    "            if type(mean_val_pred)==int:\n",
    "                mean_val_pred=(all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).clone().detach().cpu().numpy()\n",
    "            else:\n",
    "                mean_val_pred=np.concatenate((mean_val_pred,\n",
    "                                    (all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).clone().detach().cpu().numpy(),\n",
    "                                    ),axis=0)\n",
    "            if type(val_tar)==int:\n",
    "                val_tar=targets_.clone().detach().cpu().numpy()\n",
    "            else:\n",
    "                val_tar=np.concatenate((val_tar,\n",
    "                                    targets_.clone().detach().cpu().numpy(),\n",
    "                                    ),axis=0)\n",
    "    mean_macro_auc=roc_auc_score(val_tar, mean_val_pred, average='macro')\n",
    "    print('mean_macro_auc= ',mean_macro_auc)\n",
    "    if scheduler!=None:\n",
    "        scheduler.step(mean_macro_auc)    \n",
    "    ema.restore()\n",
    "    return mean_macro_auc\n",
    "print('ok')\n",
    "\n",
    "def test_one_epoch_wEMA(epoch, model,loss_fn,test_loader, device):\n",
    "    global ema\n",
    "    ema.apply_shadow()\n",
    "    model.eval() \n",
    "    test_tar=0\n",
    "    mean_test_pred=0\n",
    "    max_test_pred=0\n",
    "    with torch.no_grad():\n",
    "        t = time.time()\n",
    "        running_loss = None \n",
    "        pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "        for step, (in_data_, targets_) in pbar:#bs,1000，12    bs,44\n",
    "            data_=in_data_[:,:1000,:].transpose(-1,-2).type(torch.FloatTensor).to(device)#12，1000\n",
    "            meta_=in_data_[:,1000:,:].transpose(-1,-2).type(torch.FloatTensor).to(device)#12，4\n",
    "            all_seg_preds_=torch.zeros((ceil((data_.shape[-1])/125-0.1)-1,targets_.shape[0],targets_.shape[1])).type(torch.FloatTensor).to(device)  #7，bs，44\n",
    "            for tmp_seg in range(ceil((data_.shape[-1])/125-0.1)-1):\n",
    "                seg_data_=torch.cat((data_[:,:,125*tmp_seg:125*(tmp_seg+2)],meta_),-1)        \n",
    "                seg_preds_=model(seg_data_).type(torch.cuda.FloatTensor)# bs,44\n",
    "                all_seg_preds_[tmp_seg,:,:]=seg_preds_.clone().detach().type(torch.cuda.FloatTensor) # bs,44\n",
    "            agg_preds_=(all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).to(device)\n",
    "            targets_=targets_.type(torch.FloatTensor).to(device) # bs,1000,12\n",
    "            loss=loss_fn(agg_preds_,targets_)\n",
    "            if running_loss is None:\n",
    "                running_loss_list=[]\n",
    "                running_loss = loss.item()\n",
    "                running_loss_list.append(loss.item()) \n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01 \n",
    "                running_loss_list.append(loss.item()) \n",
    "            description = f'epoch {epoch} mean loss: {mean(running_loss_list):.4f}' \n",
    "            pbar.set_description(description)  \n",
    "            all_seg_preds_=torch.sigmoid(all_seg_preds_)\n",
    "            if type(mean_test_pred)==int:\n",
    "                mean_test_pred=(all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).clone().detach().cpu().numpy()\n",
    "            else:\n",
    "                mean_test_pred=np.concatenate((mean_test_pred,\n",
    "                                    (all_seg_preds_.mean(dim=0)).type(torch.FloatTensor).clone().detach().cpu().numpy(),\n",
    "                                    ),axis=0)\n",
    "            if type(test_tar)==int:\n",
    "                test_tar=targets_.clone().detach().cpu().numpy()\n",
    "            else:\n",
    "                test_tar=np.concatenate((test_tar,\n",
    "                                    targets_.clone().detach().cpu().numpy(),\n",
    "                                    ),axis=0)\n",
    "    mean_macro_auc=roc_auc_score(test_tar, mean_test_pred, average='macro')\n",
    "    print('mean_macro_auc= ',mean_macro_auc)\n",
    "    ema.restore()\n",
    "    return mean_macro_auc\n",
    "print('ok')\n",
    "\n",
    "\n",
    "class GetLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_data, data_label):\n",
    "        self.data = data_data\n",
    "        self.label = data_label\n",
    "    def __getitem__(self, index):\n",
    "        data=self.data[index]\n",
    "        labels=self.label[index]\n",
    "        return data, labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "origin_data_train=X_train\n",
    "label_train=y_train\n",
    "origin_data_val=X_val\n",
    "label_val=y_val\n",
    "origin_data_test=X_test\n",
    "label_test=y_test\n",
    "\n",
    "print('load ok')\n",
    "\n",
    "data_train=origin_data_train\n",
    "data_val=origin_data_val\n",
    "data_test=origin_data_test\n",
    "\n",
    "if mytask=='form':\n",
    "    # 'The task has a smaller amount of data. To achieve the unification of the process, we copy the data to obtain similar training data.\n",
    "    data_train=np.concatenate((data_train,data_train),axis=0)\n",
    "    label_train=np.concatenate((label_train,label_train),axis=0)\n",
    "\n",
    "print('data_val.shape=',data_val.shape)\n",
    "print('data_test.shape=',data_test.shape)\n",
    "print('data_train.shape=',data_train.shape)\n",
    "print('label_train.shape=',label_train.shape)\n",
    "print('label_val.shape=',label_val.shape)\n",
    "print('label_test.shape=',label_test.shape)\n",
    "bs1=128\n",
    "bs2=128\n",
    "bs3=128\n",
    "\n",
    "train_datasloader = DataLoader(GetLoader(data_train, label_train), bs1, shuffle=True, drop_last=False, num_workers=2)\n",
    "val_datasloader = DataLoader(GetLoader(data_val, label_val), bs2, shuffle=False, drop_last=False, num_workers=2)\n",
    "test_datasloader = DataLoader(GetLoader(data_test, label_test), bs3, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "#EMA\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        #self.waive_epoch=waive_epoch\n",
    "        self.count=0\n",
    "        \n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                tmp_decay=self.decay\n",
    "                self.count+=1    \n",
    "                new_average = (1.0 - tmp_decay) * param.data + tmp_decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "    \n",
    "    def adjust_lr_to(self,a:float):\n",
    "        self.param_groups[0]['lr']=a\n",
    "    \n",
    "\n",
    "ECGViT_model=ECGViT(num_classes = label_train.shape[-1],dim = 160,mlp_dim = 480,Da_depth=4, F_depth=2,patch_size=50,leads=12,dropout=0.2,meta_info=4).to(device) \n",
    "\n",
    "global ema\n",
    "train_loss = []  \n",
    "val_mean_loss = []\n",
    "test_macro_auc=[]\n",
    "ema = EMA(ECGViT_model, 0.998)#, waive_epoch=args['warm_epoch'])\n",
    "ema.register()\n",
    "loss_fn=F.binary_cross_entropy_with_logits\n",
    "recommand_lr=3.E-3\n",
    "print('recommand_lr=',recommand_lr)\n",
    "\n",
    "# warmup\n",
    "warm_epoch=20\n",
    "total_epoch=60\n",
    "beta2=0.99\n",
    "finetune_weight_decay=0.05\n",
    "base_optimizer=torch.optim.AdamW\n",
    "optimizer = SAM(ECGViT_model.parameters(),base_optimizer,lr=recommand_lr, betas=(0.9, beta2), eps=1e-08, weight_decay=finetune_weight_decay)\n",
    "scheduler = None   \n",
    "for epoch in range (warm_epoch):\n",
    "    print('current lr=',recommand_lr*(epoch+1)/warm_epoch)\n",
    "    optimizer.adjust_lr_to(recommand_lr*(epoch+1)/warm_epoch)\n",
    "    t_loss=train_one_epoch(epoch, ECGViT_model, loss_fn, optimizer, train_datasloader, device,scheduler,warmup=True)\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "# train\n",
    "\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=0,min_lr=0, threshold=1e-7, threshold_mode='rel',eps=1e-5, verbose=True)\n",
    "\n",
    "best_val_w=0.\n",
    "best_epoch_o=0\n",
    "final_test_w=0.\n",
    "best_val_o=0.\n",
    "best_epoch_o=0\n",
    "final_test_o=0.\n",
    "conlow=0\n",
    "\n",
    "for epoch in range(warm_epoch,total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    t_loss=train_one_epoch(epoch, ECGViT_model, loss_fn, optimizer, train_datasloader, device)\n",
    "    train_loss.append(t_loss)\n",
    "    wEMA_val_AUC=val_one_epoch_wEMA(epoch, ECGViT_model, loss_fn,val_datasloader, device,scheduler) \n",
    "    wEMA_test_AUC=test_one_epoch_wEMA(epoch, ECGViT_model, loss_fn,test_datasloader, device)   \n",
    "    if (best_val_w<wEMA_val_AUC):\n",
    "        conlow=0\n",
    "        best_val_w=wEMA_val_AUC\n",
    "        best_epoch_w=epoch\n",
    "        final_test_w=wEMA_test_AUC\n",
    "        print('find better model w') \n",
    "    else:\n",
    "        conlow+=1\n",
    "    print('wEMA_val_AUC=',wEMA_val_AUC,'   --')\n",
    "    print('wEMA_test_AUC=',wEMA_test_AUC,'   --')\n",
    "    torch.cuda.empty_cache()    \n",
    "    if conlow==3:\n",
    "        print('The score of validation set is not improved in three consecutive epochs, triggering the early stopping condition.')\n",
    "        break\n",
    "print('found best model w in epoch: ',best_epoch_w)    \n",
    "print('best_val_w=',best_val_w)\n",
    "print('final_test_w=',final_test_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5bc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
